# Databricks notebook source
# spark is spark session
# spark session is entry point for spark sql/dataframe-df
# any spark application can have 1 or more spark sessions
# any spark application shall have 1 Spark context
# if we create spark session, it will create spark context first if context not created..
# spark session can be extended with hive catalog [meta database, database,  tables, colunes/data types, location of data, but data is not stored in HIVE], data may be stord in data lake like hdfs, s3, ADLS etc

spark # databricks spark session enabled with hive, mean it can use spark database via hive

# COMMAND ----------

spark.sparkContext # spark context

# COMMAND ----------

# spark is spark session helps to create dataframe
# spark context helps to create RDD
# create dataframe using spark, 1 record, 2 columns, data type is inferred by spark
df = spark.createDataFrame(data=[(10, 'F')], schema=["age","gender"])
df.printSchema()
df.show()

# COMMAND ----------

# dataframes are immutable
# every DF has rdd internally
# RDD of type Row
df.rdd.collect() # Row

# COMMAND ----------

df.rdd.getNumPartitions()

# COMMAND ----------

df.rdd.glom().collect()

# COMMAND ----------

# dataframe, rdd, spark sql are known only to Driver not to executor
# executor still deal with Task and Partitions , task code for executor generated by CAtalyst engine
# driver has the catalyst engine that generate code, send code to executor as task to run


# COMMAND ----------

# df has schema
df.schema 

# COMMAND ----------

df.take(1) # read from 1st partition for n records, same as df.rdd.take(1)

# COMMAND ----------

df.rdd.take(1)

# COMMAND ----------

# write data frame api code
# DATA FRAME API For filter.. catalyst will generate an rdd function for filter and execute
df2 = df.filter (df.gender == 'F')
df2.show()

# COMMAND ----------

# developers also can write low level RDD code on Dataframe
df.rdd.filter(lambda row: row['gender'] == 'F').collect()


# COMMAND ----------

